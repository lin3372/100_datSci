{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "150_Naïve Bayes Algorithm (Math).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP15DnffrNqeC7k3xe/Rg/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lin3372/100_datSci/blob/main/05_naive_bayes/150_Na%C3%AFve_Bayes_Algorithm_(Math).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF3cMeL4ZJeV"
      },
      "outputs": [],
      "source": [
        "#2112121008"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naïve Bayes Algorithm - Math Foundation\n",
        "Exploring Naive Bayes: Mathematics, How it works, Pros & Cons, and Applications"
      ],
      "metadata": {
        "id": "0NKvJXMtZd2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Naïve Bayes Algorithm?"
      ],
      "metadata": {
        "id": "tf2M5iaFZw1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a classification technique that is based on Bayes’ Theorem with an assumption that all the features that predicts the target value are independent of each other. \n",
        "\n",
        "**It calculates the probability of each class and then pick the one with the highest probability.** \n",
        "**Bayes’ Theorem describes the probability of an event, based on a prior knowledge of conditions that might be related to that event.**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1200/1*aFhOj7TdBIZir4keHMgHOw.png\" width=\"500\" align=\"center\">\n",
        "\n",
        "**Naive Bayes classifier assumes that the features we use to predict the target are independent and do not affect each other**. While in real-life data, features depend on each other in determining the target, but this is ignored by the Naive Bayes classifier.\n",
        "**Though the independence assumption is never correct in real-world data, but often works well in practice. so that it is called “Naive”**."
      ],
      "metadata": {
        "id": "onF44jHWZsni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Math behind Naive Bays Algorithm**\n",
        "Given a features vector X=(x1,x2,…,xn) and a class variable y, Bayes Theorem states that:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/954/1*xfvAipyVBfo1RPwkL9DmYQ.png\" width = \"400\" align=\"center\">\n",
        "\n",
        "We’re interested in calculating the **posterior probability** ***P(y | X)*** from the likelihood ***P(X | y)*** and prior probabilities ***P(y)***,***P(X)***."
      ],
      "metadata": {
        "id": "5IOLrPCMbw3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the chain rule, the likelihood ***P(X ∣ y)*** can be decomposed as:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1340/1*7E_s6icZgyWvf-Vm4OhtFg.png\" align=\"center\" width=\"400\" >\n",
        "\n",
        "Because of the **Naive’s conditional independence assumption**, the conditional probabilities are independent of each other.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1050/1*n2LbIxG16jQFaSmBzL3oLQ.png\" align=\"center\" width=\"400\">\n",
        "\n",
        "Thus, by conditional independence, we have:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*bIZ1Jdn3k5deoeJxwNxRmA.png\" align=\"center\" width=\"400\">"
      ],
      "metadata": {
        "id": "PEjHFBR1gs73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes classifier**\n",
        "And as denominator remains constant for all values, the posterior probability can then be:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1252/1*FFmYvf4oZ9MafJXOfWN1mA.png\" align=\"center\" width=\"400\">\n",
        "\n",
        "The **Naive Bayes classifier** combines this model with a decision rule. One common rule is to pick the hypothesis that’s most probable; this is known as the maximum a posteriori or MAP decision rule.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1016/1*eD4a7MigslH6n4QtOARsEw.png\" align=\"center\" width=\"400\">"
      ],
      "metadata": {
        "id": "Bbi5PyNDhy6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Email Spam Filter example"
      ],
      "metadata": {
        "id": "SK5xJ-_cjQhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a bunch of emails that we want to classify as spam or not spam\n",
        "\n",
        "**Problem**: Assuming we have the message “Hello friend” and we want to know whether it is a spam or not based on the data we collected below\n",
        "\n",
        "Our dataset has 15 Not Spam emails and 10 Spam emails. Some analysis had been done, and the frequency of each word had been recorded as shown below:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/908/1*G-g9VQJzCG0ksqPpHQ247g.png\" width=280> \n",
        "\n"
      ],
      "metadata": {
        "id": "INZllDRjjhUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do some exploration for some probabilities:\n",
        "*   P(Dear|Not Spam) = 8/34\n",
        "*   P(Visit|Not Spam) = 2/34\n",
        "*   P(Dear|Spam) = 3/47\n",
        "*   P(Visit|Spam) = 6/47"
      ],
      "metadata": {
        "id": "OA5r-rOtkJLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By using Bayes’ Theorem**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*PharzCCYWYPV-2jK9FBkKQ.png\" width=\"600\">, ignoring the denominator, we have \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*P15GjWNcnQ6z4WtQn06Z8g.png\" width=\"600\">"
      ],
      "metadata": {
        "id": "t5qcNadllHxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes assumes that the **features we use to predict the target are independent**. so,\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*ogobemBLzfW7JnQs1u6kCw.png\" width=\"680\">\n",
        "\n",
        "Likewise, we can calculate the probability of being spam using the same procedure:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Jnn2iV_TmAvvvrZN1BUhmw.png\" width=\"680\">\n",
        "\n",
        "So, the message **“Hello friend” is not a spam**."
      ],
      "metadata": {
        "id": "q_vexr1_l6HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Email Spam Filter example2 (Zero Freq Problem)\n",
        "Let’s try another example -- the message “dear visit dinner money money money”. \n",
        "\n",
        "It’s obvious that it’s a spam, but let’s see what Naive Bayes will say. "
      ],
      "metadata": {
        "id": "5L8cyAFgoPsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*vvgrfid5BYTzha36CeZqJg.png\" width = \"500\">\n",
        "\n",
        "===============================\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*bpCJ8Y_PWyFlxObt4Xhbgg.png\" width = \"600\" align=\"left\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Q09-CyV_Zo04I0jLMbSjtA.png\" width = \"600\" align=\"left\">\n",
        "\n"
      ],
      "metadata": {
        "id": "TkPAJwntohrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "oops!! Naive Bays says that this message is not a spam?!!!"
      ],
      "metadata": {
        "id": "Re1rUMx0p_Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Frequency Problem and Laplace smoothing\n",
        "\n",
        "\n",
        "> **Laplace Smoothing** is a technique for **smoothing categorical data**. A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Hence, no probability will be zero. this is a way of **regularizing Naive Bayes**.\n",
        "\n",
        "> Given an observation ***x = (x1, …, xd)*** from a multinomial distribution with ***N*** trials and parameter vector ***θ = (θ1, …, θd)***, a “smoothed” version of the data gives the estimator:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Do1OmxJOvszQ9KCujg5fEg.png\" width=\"600\">\n",
        "\n",
        "\n",
        "> where the pseudo-count ***α > 0*** is the smoothing parameter (***α ***= 0 corresponds to no smoothing).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JN9YU580pyYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to our problem, we are going to pick ***α = 1***, and ‘***d***’ is *the number of the unique words in the datase*t which is 10 in our case.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*RvDH6YKDKDnn_EUZfuuZOQ.png\" width=\"500\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*4wm4r-4MRgyuXNC-Kq6l8Q.png\" width=\"600\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*bfohzjLKPUve_kui8EL53A.png\" width=\"600\">\n",
        "\n",
        "Now it’s correctly classified the message as spam\n"
      ],
      "metadata": {
        "id": "MgGR0a1hr0iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Types of Naive Bayes Classifiers**\n",
        "\n",
        "1.  **Multinomial**: Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. For example, the count how often each word occurs in the document. This is the event model typically used for document classification.\n",
        "\n",
        "2.  **Bernoulli**: Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence(i.e. a word occurs in a document or not) features are used rather than term frequencies(i.e. frequency of a word in the document).\n",
        "\n",
        "3.  **Gaussian**: It is used in classification and it assumes that features follow a normal distribution."
      ],
      "metadata": {
        "id": "to1njBa-siub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pros and Cons for Naive Bayes\n",
        "> **Pros:**\n",
        "1.   Requires a small amount of training data. So the training takes less time.\n",
        "2.   Handles continuous and discrete data, and it is not sensitive to irrelevant features.\n",
        "3.   Very simple, fast, and easy to implement.\n",
        "4.   Can be used for both binary and multi-class classification problems.\n",
        "5.   Highly scalable as it scales linearly with the number of predictor features and data points.\n",
        "6.   When the Naive Bayes conditional independence assumption holds true, it will converge quicker than discriminative models like logistic regression.\n",
        "\n",
        ">**Cons:**\n",
        "1.   The assumption of independent predictors/features. Naive Bayes implicitly assumes that all the attributes are mutually independent which is almost impossible to find in real-world data.\n",
        "2.   If a categorical variable has a value that appears in the test dataset, and not observed in the training dataset, then the model will assign it a zero probability and will not be able to make a prediction. This is what we called the “**Zero Frequency problem**“, and can be solved using **smoothing techniques**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pHDRwNX2s3Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of Naive Bayes Algorithm\n",
        "\n",
        "\n",
        "*   Real-time Prediction.\n",
        "*   Multi-class Prediction.\n",
        "*   Text classification/ Spam Filtering/ Sentiment Analysis.\n",
        "*   Recommendation Systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "WoEIuXq4tp06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "1. (medium) [Naïve Bayes Algorithm](https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2)\n",
        "\n",
        "2.   Additive Smoothing  [[here]](https://en.wikipedia.org/wiki/Additive_smoothing)"
      ],
      "metadata": {
        "id": "uTkVGwo4cush"
      }
    }
  ]
}